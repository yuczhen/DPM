# -*- coding: utf-8 -*-
"""
DBè¨˜éŒ„æ¨¡çµ„
ç”¨æ–¼å°‡å¯¦é©—çµæœå¯«å…¥è³‡æ–™åº«ï¼Œæ–¹ä¾¿å¾ŒçºŒè§€å¯Ÿå’Œåˆ†æ
"""

import sqlite3
import pandas as pd
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
import os

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    load_dotenv()  # è¼‰å…¥ .env æª”æ¡ˆ
    print("âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥æˆåŠŸ (DB Logger)")
except ImportError:
    print("âš ï¸ python-dotenv æœªå®‰è£ï¼Œä½¿ç”¨ç³»çµ±ç’°å¢ƒè®Šæ•¸")

class ExperimentDBLogger:
    """
    å¯¦é©—è³‡æ–™åº«è¨˜éŒ„å™¨
    å°‡æ¨¡å‹è¨“ç·´çµæœè¨˜éŒ„åˆ°SQLiteè³‡æ–™åº«
    """

    def __init__(self, db_path: str = None):
        """
        åˆå§‹åŒ–DBè¨˜éŒ„å™¨

        Args:
            db_path (str): è³‡æ–™åº«æª”æ¡ˆè·¯å¾‘ï¼Œå¦‚æœç‚ºNoneå‰‡å¾ç’°å¢ƒè®Šæ•¸è®€å–
        """
        if db_path is None:
            # å¾ç’°å¢ƒè®Šæ•¸å–å¾—è³‡æ–™åº«è·¯å¾‘
            db_path = os.getenv('DATABASE_PATH', 'experiments.db')

        self.db_path = db_path
        self.db_backup_path = os.getenv('DATABASE_BACKUP_PATH', 'backups/experiments_backup.db')

        # ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(self.db_backup_path), exist_ok=True)

        self._init_db()

    def backup_database(self) -> bool:
        """
        å‚™ä»½è³‡æ–™åº«åˆ°æŒ‡å®šè·¯å¾‘

        Returns:
            bool: æ˜¯å¦æˆåŠŸå‚™ä»½
        """
        try:
            import shutil

            # ç¢ºä¿å‚™ä»½ç›®éŒ„å­˜åœ¨
            backup_dir = os.path.dirname(self.db_backup_path)
            if backup_dir and not os.path.exists(backup_dir):
                os.makedirs(backup_dir)

            # è¤‡è£½è³‡æ–™åº«æª”æ¡ˆ
            if os.path.exists(self.db_path):
                shutil.copy2(self.db_path, self.db_backup_path)
                print(f"âœ… è³‡æ–™åº«å‚™ä»½æˆåŠŸ: {self.db_backup_path}")
                return True
            else:
                print(f"âŒ è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨: {self.db_path}")
                return False

        except Exception as e:
            print(f"âŒ è³‡æ–™åº«å‚™ä»½å¤±æ•—: {e}")
            return False

    def _init_db(self):
        """åˆå§‹åŒ–è³‡æ–™åº«å’Œè¡¨æ ¼"""
        conn = sqlite3.connect(self.db_path)

        # å‰µå»ºå¯¦é©—è¨˜éŒ„è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS experiments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                experiment_id TEXT NOT NULL UNIQUE,
                model_name TEXT,
                dataset_version TEXT,
                train_samples INTEGER,
                val_samples INTEGER,
                test_samples INTEGER,
                train_period_start TEXT,
                train_period_end TEXT,
                val_period_start TEXT,
                val_period_end TEXT,
                test_period_start TEXT,
                test_period_end TEXT,
                features_used TEXT,
                metrics TEXT,
                wandb_run_id TEXT,
                created_at TEXT,
                updated_at TEXT,
                notes TEXT
            )
        ''')

        # å‰µå»ºæŒ‡æ¨™æ­·å²è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS metrics_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                experiment_id TEXT NOT NULL,
                metric_name TEXT NOT NULL,
                metric_value REAL NOT NULL,
                recorded_at TEXT,
                FOREIGN KEY (experiment_id) REFERENCES experiments (experiment_id)
            )
        ''')

        # å‰µå»ºæ¨¡å‹æ€§èƒ½è¿½è¹¤è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS model_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                experiment_id TEXT NOT NULL,
                model_version TEXT,
                accuracy REAL,
                precision REAL,
                recall REAL,
                f1_score REAL,
                auc_roc REAL,
                log_loss REAL,
                evaluated_at TEXT,
                FOREIGN KEY (experiment_id) REFERENCES experiments (experiment_id)
            )
        ''')

        conn.commit()
        conn.close()
        print(f"âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ: {self.db_path}")

    def log_experiment(self, experiment_record: Dict[str, Any]) -> bool:
        """
        è¨˜éŒ„å¯¦é©—çµæœåˆ°è³‡æ–™åº«

        Args:
            experiment_record (dict): å¯¦é©—è¨˜éŒ„

        Returns:
            bool: æ˜¯å¦æˆåŠŸè¨˜éŒ„
        """
        try:
            conn = sqlite3.connect(self.db_path)

            # æº–å‚™è³‡æ–™
            record = {
                'experiment_id': experiment_record.get('experiment_id', f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
                'model_name': experiment_record.get('model_name', 'Unknown'),
                'dataset_version': experiment_record.get('dataset_version', 'v1.0'),
                'train_samples': experiment_record.get('train_samples', 0),
                'val_samples': experiment_record.get('val_samples', 0),
                'test_samples': experiment_record.get('test_samples', 0),
                'train_period_start': experiment_record.get('train_period_start'),
                'train_period_end': experiment_record.get('train_period_end'),
                'val_period_start': experiment_record.get('val_period_start'),
                'val_period_end': experiment_record.get('val_period_end'),
                'test_period_start': experiment_record.get('test_period_start'),
                'test_period_end': experiment_record.get('test_period_end'),
                'features_used': json.dumps(experiment_record.get('features_used', [])),
                'metrics': json.dumps(experiment_record.get('metrics', {})),
                'wandb_run_id': experiment_record.get('wandb_run_id'),
                'created_at': experiment_record.get('created_at', datetime.now().isoformat()),
                'updated_at': experiment_record.get('updated_at', datetime.now().isoformat()),
                'notes': experiment_record.get('notes', '')
            }

            # æ’å…¥è¨˜éŒ„
            columns = ', '.join(record.keys())
            placeholders = ', '.join(['?' for _ in record.values()])
            values = list(record.values())

            query = f'''
                INSERT OR REPLACE INTO experiments
                ({columns})
                VALUES ({placeholders})
            '''

            conn.execute(query, values)

            # è¨˜éŒ„è©³ç´°æŒ‡æ¨™
            if 'metrics' in experiment_record:
                metrics = experiment_record['metrics']
                experiment_id = record['experiment_id']

                for metric_name, metric_value in metrics.items():
                    if isinstance(metric_value, (int, float)):
                        conn.execute('''
                            INSERT INTO metrics_history
                            (experiment_id, metric_name, metric_value, recorded_at)
                            VALUES (?, ?, ?, ?)
                        ''', (experiment_id, metric_name, float(metric_value), datetime.now().isoformat()))

            conn.commit()
            conn.close()

            print(f"âœ… å¯¦é©—è¨˜éŒ„æˆåŠŸ: {record['experiment_id']}")
            return True

        except Exception as e:
            print(f"âŒ è¨˜éŒ„å¯¦é©—å¤±æ•—: {e}")
            return False

    def log_model_performance(self, experiment_id: str, model_metrics: Dict[str, float]) -> bool:
        """
        è¨˜éŒ„æ¨¡å‹æ€§èƒ½æŒ‡æ¨™

        Args:
            experiment_id (str): å¯¦é©—ID
            model_metrics (dict): æ¨¡å‹æ€§èƒ½æŒ‡æ¨™

        Returns:
            bool: æ˜¯å¦æˆåŠŸè¨˜éŒ„
        """
        try:
            conn = sqlite3.connect(self.db_path)

            record = {
                'experiment_id': experiment_id,
                'model_version': model_metrics.get('model_version', 'v1.0'),
                'accuracy': model_metrics.get('accuracy'),
                'precision': model_metrics.get('precision'),
                'recall': model_metrics.get('recall'),
                'f1_score': model_metrics.get('f1_score'),
                'auc_roc': model_metrics.get('auc_roc'),
                'log_loss': model_metrics.get('log_loss'),
                'evaluated_at': datetime.now().isoformat()
            }

            # ç§»é™¤Noneå€¼
            record = {k: v for k, v in record.items() if v is not None}

            columns = ', '.join(record.keys())
            placeholders = ', '.join(['?' for _ in record.values()])
            values = list(record.values())

            query = f'''
                INSERT INTO model_performance
                ({columns})
                VALUES ({placeholders})
            '''

            conn.execute(query, values)
            conn.commit()
            conn.close()

            print(f"âœ… æ¨¡å‹æ€§èƒ½è¨˜éŒ„æˆåŠŸ: {experiment_id}")
            return True

        except Exception as e:
            print(f"âŒ è¨˜éŒ„æ¨¡å‹æ€§èƒ½å¤±æ•—: {e}")
            return False

    def get_experiment_history(self, limit: int = 10) -> pd.DataFrame:
        """
        ç²å–å¯¦é©—æ­·å²è¨˜éŒ„

        Args:
            limit (int): è¨˜éŒ„æ•¸é‡é™åˆ¶

        Returns:
            pd.DataFrame: å¯¦é©—æ­·å²
        """
        try:
            conn = sqlite3.connect(self.db_path)

            query = f'''
                SELECT
                    experiment_id,
                    model_name,
                    dataset_version,
                    train_samples + val_samples + test_samples as total_samples,
                    metrics,
                    created_at
                FROM experiments
                ORDER BY created_at DESC
                LIMIT {limit}
            '''

            df = pd.read_sql_query(query, conn)
            conn.close()

            # è§£æmetrics JSON
            if not df.empty:
                df['metrics'] = df['metrics'].apply(lambda x: json.loads(x) if x else {})

            return df

        except Exception as e:
            print(f"âŒ ç²å–å¯¦é©—æ­·å²å¤±æ•—: {e}")
            return pd.DataFrame()

    def get_performance_trend(self, metric_name: str = 'auc_roc') -> pd.DataFrame:
        """
        ç²å–æ€§èƒ½è¶¨å‹¢

        Args:
            metric_name (str): æŒ‡æ¨™åç¨±

        Returns:
            pd.DataFrame: æ€§èƒ½è¶¨å‹¢
        """
        try:
            conn = sqlite3.connect(self.db_path)

            query = '''
                SELECT
                    e.experiment_id,
                    e.model_name,
                    e.created_at,
                    mh.metric_value,
                    mh.metric_name
                FROM experiments e
                JOIN metrics_history mh ON e.experiment_id = mh.experiment_id
                WHERE mh.metric_name = ?
                ORDER BY e.created_at
            '''

            df = pd.read_sql_query(query, conn, params=(metric_name,))
            conn.close()

            return df

        except Exception as e:
            print(f"âŒ ç²å–æ€§èƒ½è¶¨å‹¢å¤±æ•—: {e}")
            return pd.DataFrame()

    def create_performance_dashboard_data(self) -> Dict[str, Any]:
        """
        å‰µå»ºæ€§èƒ½å„€è¡¨æ¿è³‡æ–™

        Returns:
            dict: å„€è¡¨æ¿è³‡æ–™
        """
        try:
            conn = sqlite3.connect(self.db_path)

            # æœ€æ–°å¯¦é©—æŒ‡æ¨™
            latest_query = '''
                SELECT
                    e.experiment_id,
                    e.model_name,
                    e.created_at,
                    mh.metric_name,
                    mh.metric_value
                FROM experiments e
                JOIN metrics_history mh ON e.experiment_id = mh.experiment_id
                WHERE e.created_at = (
                    SELECT MAX(created_at) FROM experiments
                )
            '''

            latest_metrics = pd.read_sql_query(latest_query, conn)

            # æ€§èƒ½è¶¨å‹¢
            trend_query = '''
                SELECT
                    e.created_at,
                    AVG(mh.metric_value) as avg_auc
                FROM experiments e
                JOIN metrics_history mh ON e.experiment_id = mh.experiment_id
                WHERE mh.metric_name = 'auc_roc'
                GROUP BY e.created_at
                ORDER BY e.created_at
            '''

            performance_trend = pd.read_sql_query(trend_query, conn)

            # æ¨¡å‹æ¯”è¼ƒ
            comparison_query = '''
                SELECT
                    model_name,
                    AVG(CAST(mh.metric_value AS FLOAT)) as avg_auc,
                    COUNT(*) as experiment_count
                FROM experiments e
                JOIN metrics_history mh ON e.experiment_id = mh.experiment_id
                WHERE mh.metric_name = 'auc_roc'
                GROUP BY model_name
                ORDER BY avg_auc DESC
            '''

            model_comparison = pd.read_sql_query(comparison_query, conn)

            conn.close()

            return {
                'latest_experiment': {
                    'experiment_id': latest_metrics['experiment_id'].iloc[0] if not latest_metrics.empty else None,
                    'metrics': dict(zip(latest_metrics['metric_name'], latest_metrics['metric_value']))
                },
                'performance_trend': performance_trend.to_dict('records'),
                'model_comparison': model_comparison.to_dict('records'),
                'total_experiments': len(latest_metrics['experiment_id'].unique()) if not latest_metrics.empty else 0
            }

        except Exception as e:
            print(f"âŒ å‰µå»ºå„€è¡¨æ¿è³‡æ–™å¤±æ•—: {e}")
            return {}

    def export_experiments_to_csv(self, output_path: str = 'experiments_export.csv') -> bool:
        """
        åŒ¯å‡ºå¯¦é©—è¨˜éŒ„åˆ°CSV

        Args:
            output_path (str): è¼¸å‡ºæª”æ¡ˆè·¯å¾‘

        Returns:
            bool: æ˜¯å¦æˆåŠŸåŒ¯å‡º
        """
        try:
            df = self.get_experiment_history(limit=1000)

            if not df.empty:
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
                print(f"âœ… å¯¦é©—è¨˜éŒ„åŒ¯å‡ºæˆåŠŸ: {output_path}")
                return True
            else:
                print("âš ï¸ æ²’æœ‰å¯¦é©—è¨˜éŒ„å¯åŒ¯å‡º")
                return False

        except Exception as e:
            print(f"âŒ åŒ¯å‡ºå¤±æ•—: {e}")
            return False


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–DBè¨˜éŒ„å™¨
    logger = ExperimentDBLogger()

    # ç¯„ä¾‹å¯¦é©—è¨˜éŒ„
    example_experiment = {
        'experiment_id': 'demo_experiment_001',
        'model_name': 'XGBoost_Ensemble',
        'dataset_version': 'recent_5_years_v1',
        'train_samples': 80000,
        'val_samples': 17000,
        'test_samples': 17000,
        'train_period_start': '2019-01-01',
        'train_period_end': '2022-12-31',
        'val_period_start': '2023-01-01',
        'val_period_end': '2023-08-31',
        'test_period_start': '2023-09-01',
        'test_period_end': '2023-12-31',
        'features_used': ['age', 'income', 'credit_history', 'payment_behavior'],
        'metrics': {
            'accuracy': 0.892,
            'precision': 0.845,
            'recall': 0.782,
            'f1_score': 0.812,
            'auc_roc': 0.923,
            'log_loss': 0.287
        },
        'wandb_run_id': 'demo_run_123',
        'notes': 'æ™‚é–“æ„è­˜è¨“ç·´çš„ç¬¬ä¸€å€‹å¯¦é©—'
    }

    # è¨˜éŒ„å¯¦é©—
    logger.log_experiment(example_experiment)

    # è¨˜éŒ„æ¨¡å‹æ€§èƒ½
    model_metrics = {
        'model_version': 'v1.0',
        'accuracy': 0.892,
        'precision': 0.845,
        'recall': 0.782,
        'f1_score': 0.812,
        'auc_roc': 0.923,
        'log_loss': 0.287
    }
    logger.log_model_performance('demo_experiment_001', model_metrics)

    # ç²å–æ­·å²è¨˜éŒ„
    history = logger.get_experiment_history(limit=5)
    print("\nğŸ“Š å¯¦é©—æ­·å²:")
    print(history)

    # åŒ¯å‡ºåˆ°CSV
    logger.export_experiments_to_csv()

        print("\nâœ… DBè¨˜éŒ„ç¯„ä¾‹å®Œæˆï¼")


# =============================================
# Prediction Model å°ˆç”¨ DB Logger
# =============================================

class PredictionDBLogger:
    """
    é æ¸¬æ¨¡å‹å°ˆç”¨è³‡æ–™åº«è¨˜éŒ„å™¨
    ç”¨æ–¼è¨˜éŒ„å³æ™‚é æ¸¬çµæœå’Œæ¨¡å‹ç›£æ§
    """

    def __init__(self, db_path: str = None):
        """
        åˆå§‹åŒ–é æ¸¬DBè¨˜éŒ„å™¨

        Args:
            db_path (str): è³‡æ–™åº«æª”æ¡ˆè·¯å¾‘ï¼Œå¦‚æœç‚ºNoneå‰‡å¾ç’°å¢ƒè®Šæ•¸è®€å–
        """
        if db_path is None:
            # å¾ç’°å¢ƒè®Šæ•¸å–å¾—è³‡æ–™åº«è·¯å¾‘
            db_path = os.getenv('DATABASE_PATH', 'experiments.db')

        self.db_path = db_path
        self._init_prediction_tables()

    def _init_prediction_tables(self):
        """åˆå§‹åŒ–é æ¸¬ç›¸é—œçš„è¡¨æ ¼"""
        conn = sqlite3.connect(self.db_path)

        # å‰µå»ºé æ¸¬è¨˜éŒ„è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                client_id TEXT NOT NULL,
                prediction_timestamp TEXT NOT NULL,
                model_version TEXT,
                default_probability REAL,
                risk_category TEXT,
                risk_score INTEGER,
                model_predictions TEXT,  -- JSONæ ¼å¼çš„å¤šæ¨¡å‹é æ¸¬çµæœ
                features_used TEXT,      -- JSONæ ¼å¼çš„ç‰¹å¾µå€¼
                processing_time REAL,    -- é æ¸¬è™•ç†æ™‚é–“(ç§’)
                batch_id TEXT,           -- æ‰¹æ¬¡ID
                created_at TEXT
            )
        ''')

        # å‰µå»ºæ¨¡å‹ç›£æ§è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS model_monitoring (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_version TEXT NOT NULL,
                metric_name TEXT NOT NULL,
                metric_value REAL NOT NULL,
                recorded_at TEXT,
                alert_triggered BOOLEAN DEFAULT FALSE,
                alert_message TEXT
            )
        ''')

        # å‰µå»ºé æ¸¬çµ±è¨ˆè¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS prediction_stats (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT NOT NULL,
                total_predictions INTEGER,
                avg_default_probability REAL,
                high_risk_count INTEGER,
                medium_risk_count INTEGER,
                low_risk_count INTEGER,
                processing_errors INTEGER,
                UNIQUE(date)
            )
        ''')

        conn.commit()
        conn.close()
        print(f"âœ… é æ¸¬è³‡æ–™åº«è¡¨æ ¼åˆå§‹åŒ–å®Œæˆ: {self.db_path}")

    def log_prediction(self, prediction_record: Dict[str, Any]) -> bool:
        """
        è¨˜éŒ„å–®æ¬¡é æ¸¬çµæœ

        Args:
            prediction_record (dict): é æ¸¬è¨˜éŒ„

        Returns:
            bool: æ˜¯å¦æˆåŠŸè¨˜éŒ„
        """
        try:
            conn = sqlite3.connect(self.db_path)

            record = {
                'client_id': prediction_record.get('client_id', 'UNKNOWN'),
                'prediction_timestamp': prediction_record.get('prediction_timestamp', datetime.now().isoformat()),
                'model_version': prediction_record.get('model_version', 'v1.0'),
                'default_probability': prediction_record.get('default_probability', 0.0),
                'risk_category': prediction_record.get('risk_category', 'UNKNOWN'),
                'risk_score': prediction_record.get('risk_score', 0),
                'model_predictions': json.dumps(prediction_record.get('model_predictions', {})),
                'features_used': json.dumps(prediction_record.get('features_used', {})),
                'processing_time': prediction_record.get('processing_time', 0.0),
                'batch_id': prediction_record.get('batch_id'),
                'created_at': datetime.now().isoformat()
            }

            columns = ', '.join(record.keys())
            placeholders = ', '.join(['?' for _ in record.values()])
            values = list(record.values())

            query = f'''
                INSERT INTO predictions
                ({columns})
                VALUES ({placeholders})
            '''

            conn.execute(query, values)
            conn.commit()
            conn.close()

            print(f"âœ… é æ¸¬è¨˜éŒ„æˆåŠŸ: {record['client_id']}")
            return True

        except Exception as e:
            print(f"âŒ é æ¸¬è¨˜éŒ„å¤±æ•—: {e}")
            return False

    def log_batch_predictions(self, batch_records: List[Dict[str, Any]]) -> int:
        """
        æ‰¹æ¬¡è¨˜éŒ„é æ¸¬çµæœ

        Args:
            batch_records (list): é æ¸¬è¨˜éŒ„åˆ—è¡¨

        Returns:
            int: æˆåŠŸè¨˜éŒ„çš„æ•¸é‡
        """
        successful = 0

        for record in batch_records:
            if self.log_prediction(record):
                successful += 1

        print(f"âœ… æ‰¹æ¬¡é æ¸¬è¨˜éŒ„å®Œæˆ: {successful}/{len(batch_records)}")
        return successful

    def log_model_metric(self, model_version: str, metric_name: str, metric_value: float,
                         alert_triggered: bool = False, alert_message: str = None) -> bool:
        """
        è¨˜éŒ„æ¨¡å‹ç›£æ§æŒ‡æ¨™

        Args:
            model_version (str): æ¨¡å‹ç‰ˆæœ¬
            metric_name (str): æŒ‡æ¨™åç¨±
            metric_value (float): æŒ‡æ¨™å€¼
            alert_triggered (bool): æ˜¯å¦è§¸ç™¼è­¦å ±
            alert_message (str): è­¦å ±è¨Šæ¯

        Returns:
            bool: æ˜¯å¦æˆåŠŸè¨˜éŒ„
        """
        try:
            conn = sqlite3.connect(self.db_path)

            record = {
                'model_version': model_version,
                'metric_name': metric_name,
                'metric_value': float(metric_value),
                'recorded_at': datetime.now().isoformat(),
                'alert_triggered': alert_triggered,
                'alert_message': alert_message
            }

            columns = ', '.join(record.keys())
            placeholders = ', '.join(['?' for _ in record.values()])
            values = list(record.values())

            query = f'''
                INSERT INTO model_monitoring
                ({columns})
                VALUES ({placeholders})
            '''

            conn.execute(query, values)
            conn.commit()
            conn.close()

            if alert_triggered:
                print(f"ğŸš¨ æ¨¡å‹ç›£æ§è­¦å ±: {metric_name} = {metric_value}")
                if alert_message:
                    print(f"   è¨Šæ¯: {alert_message}")

            return True

        except Exception as e:
            print(f"âŒ æ¨¡å‹ç›£æ§è¨˜éŒ„å¤±æ•—: {e}")
            return False

    def update_prediction_stats(self, date: str = None) -> bool:
        """
        æ›´æ–°é æ¸¬çµ±è¨ˆè³‡æ–™

        Args:
            date (str): çµ±è¨ˆæ—¥æœŸï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨ä»Šå¤©

        Returns:
            bool: æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        if date is None:
            date = datetime.now().strftime('%Y-%m-%d')

        try:
            conn = sqlite3.connect(self.db_path)

            # æŸ¥è©¢ç•¶å¤©é æ¸¬çµ±è¨ˆ
            query = '''
                SELECT
                    COUNT(*) as total_predictions,
                    AVG(default_probability) as avg_default_probability,
                    SUM(CASE WHEN risk_category = 'High' THEN 1 ELSE 0 END) as high_risk_count,
                    SUM(CASE WHEN risk_category = 'Medium' THEN 1 ELSE 0 END) as medium_risk_count,
                    SUM(CASE WHEN risk_category = 'Low' THEN 1 ELSE 0 END) as low_risk_count
                FROM predictions
                WHERE DATE(prediction_timestamp) = ?
            '''

            stats = pd.read_sql_query(query, conn, params=(date,))

            if not stats.empty:
                record = {
                    'date': date,
                    'total_predictions': int(stats.iloc[0]['total_predictions']),
                    'avg_default_probability': float(stats.iloc[0]['avg_default_probability']) if stats.iloc[0]['avg_default_probability'] is not None else 0.0,
                    'high_risk_count': int(stats.iloc[0]['high_risk_count']),
                    'medium_risk_count': int(stats.iloc[0]['medium_risk_count']),
                    'low_risk_count': int(stats.iloc[0]['low_risk_count']),
                    'processing_errors': 0  # å¯ä»¥å¾å…¶ä»–åœ°æ–¹è¨ˆç®—
                }

                # ä½¿ç”¨ UPSERT èªæ³•
                conn.execute('''
                    INSERT OR REPLACE INTO prediction_stats
                    (date, total_predictions, avg_default_probability, high_risk_count,
                     medium_risk_count, low_risk_count, processing_errors)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', tuple(record.values()))

                conn.commit()
                conn.close()

                print(f"âœ… é æ¸¬çµ±è¨ˆæ›´æ–°æˆåŠŸ: {date}")
                return True
            else:
                print(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {date} çš„é æ¸¬è³‡æ–™")
                return False

        except Exception as e:
            print(f"âŒ é æ¸¬çµ±è¨ˆæ›´æ–°å¤±æ•—: {e}")
            return False

    def get_prediction_stats(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ç²å–æŒ‡å®šæœŸé–“çš„é æ¸¬çµ±è¨ˆ

        Args:
            start_date (str): é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date (str): çµæŸæ—¥æœŸ (YYYY-MM-DD)

        Returns:
            pd.DataFrame: é æ¸¬çµ±è¨ˆè³‡æ–™
        """
        try:
            conn = sqlite3.connect(self.db_path)

            query = '''
                SELECT * FROM prediction_stats
                WHERE date BETWEEN ? AND ?
                ORDER BY date
            '''

            df = pd.read_sql_query(query, conn, params=(start_date, end_date))
            conn.close()

            return df

        except Exception as e:
            print(f"âŒ ç²å–é æ¸¬çµ±è¨ˆå¤±æ•—: {e}")
            return pd.DataFrame()

    def get_recent_predictions(self, limit: int = 10) -> pd.DataFrame:
        """
        ç²å–æœ€è¿‘çš„é æ¸¬è¨˜éŒ„

        Args:
            limit (int): è¨˜éŒ„æ•¸é‡é™åˆ¶

        Returns:
            pd.DataFrame: æœ€è¿‘çš„é æ¸¬è¨˜éŒ„
        """
        try:
            conn = sqlite3.connect(self.db_path)

            query = '''
                SELECT
                    client_id,
                    prediction_timestamp,
                    default_probability,
                    risk_category,
                    risk_score,
                    model_version
                FROM predictions
                ORDER BY prediction_timestamp DESC
                LIMIT ?
            '''

            df = pd.read_sql_query(query, conn, params=(limit,))
            conn.close()

            return df

        except Exception as e:
            print(f"âŒ ç²å–æœ€è¿‘é æ¸¬å¤±æ•—: {e}")
            return pd.DataFrame()


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–DBè¨˜éŒ„å™¨
    logger = ExperimentDBLogger()

    # ç¯„ä¾‹å¯¦é©—è¨˜éŒ„
    example_experiment = {
        'experiment_id': 'demo_experiment_001',
        'model_name': 'XGBoost_Ensemble',
        'dataset_version': 'recent_5_years_v1',
        'train_samples': 80000,
        'val_samples': 17000,
        'test_samples': 17000,
        'train_period_start': '2019-01-01',
        'train_period_end': '2022-12-31',
        'val_period_start': '2023-01-01',
        'val_period_end': '2023-08-31',
        'test_period_start': '2023-09-01',
        'test_period_end': '2023-12-31',
        'features_used': ['age', 'income', 'credit_history', 'payment_behavior'],
        'metrics': {
            'accuracy': 0.892,
            'precision': 0.845,
            'recall': 0.782,
            'f1_score': 0.812,
            'auc_roc': 0.923,
            'log_loss': 0.287
        },
        'wandb_run_id': 'demo_run_123',
        'notes': 'æ™‚é–“æ„è­˜è¨“ç·´çš„ç¬¬ä¸€å€‹å¯¦é©—'
    }

    # è¨˜éŒ„å¯¦é©—
    logger.log_experiment(example_experiment)

    # å‚™ä»½è³‡æ–™åº«
    logger.backup_database()

    # è¨˜éŒ„æ¨¡å‹æ€§èƒ½
    model_metrics = {
        'model_version': 'v1.0',
        'accuracy': 0.892,
        'precision': 0.845,
        'recall': 0.782,
        'f1_score': 0.812,
        'auc_roc': 0.923,
        'log_loss': 0.287
    }
    logger.log_model_performance('demo_experiment_001', model_metrics)

    # ç²å–æ­·å²è¨˜éŒ„
    history = logger.get_experiment_history(limit=5)
    print("\nğŸ“Š å¯¦é©—æ­·å²:")
    print(history)

    # åŒ¯å‡ºåˆ°CSV
    logger.export_experiments_to_csv()

    print("\nâœ… DBè¨˜éŒ„ç¯„ä¾‹å®Œæˆï¼")

    # é æ¸¬æ¨¡å‹ç¯„ä¾‹
    print("\n=== é æ¸¬æ¨¡å‹ DB ç¯„ä¾‹ ===")
    pred_logger = PredictionDBLogger()

    # ç¯„ä¾‹é æ¸¬è¨˜éŒ„
    prediction_example = {
        'client_id': 'CLIENT_001',
        'prediction_timestamp': datetime.now().isoformat(),
        'model_version': 'v1.0',
        'default_probability': 0.15,
        'risk_category': 'Low',
        'risk_score': 750,
        'model_predictions': {
            'XGBoost': 0.12,
            'LightGBM': 0.18,
            'CatBoost': 0.15
        },
        'features_used': {
            'age': 35,
            'income': 60000,
            'credit_history': 120
        },
        'processing_time': 0.05
    }

    # è¨˜éŒ„é æ¸¬
    pred_logger.log_prediction(prediction_example)

    # è¨˜éŒ„æ¨¡å‹ç›£æ§æŒ‡æ¨™
    pred_logger.log_model_metric('v1.0', 'daily_predictions', 150)
    pred_logger.log_model_metric('v1.0', 'avg_processing_time', 0.03)

    # æ›´æ–°çµ±è¨ˆ
    pred_logger.update_prediction_stats()

    # ç²å–æœ€è¿‘é æ¸¬
    recent_preds = pred_logger.get_recent_predictions(limit=5)
    print("\nğŸ“ˆ æœ€è¿‘é æ¸¬è¨˜éŒ„:")
    print(recent_preds)

    print("\nâœ… é æ¸¬DBè¨˜éŒ„ç¯„ä¾‹å®Œæˆï¼")
